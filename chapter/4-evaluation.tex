\chapter{Evaluation}

\section{Reproduktion}

Der hier verwendete Simulations Aufbau erlaubt auf Wunsch eine deterministische sequentielle Ausführung. Dies erlaubt weiterführende Untersuchungen, die in Kapitel 4.2.2 ausgeführt werden [relativen link einfügen].


%\subsection{TLA+ Spec}
%Blabla bal
%bla
%TLA
%Mathematische spezifikation

\subsection{Simulations Aufbau}
Wahl des Paradigmas \ref{paradigma} fällt in dieser Arbeit zunächst auf Ereignis-Orientiert \ref{activity}. Dies mag erstaunen, da eine Prozess-Orientierte Ansicht, im Falle eines verteilten Systems, wie eine offensichtlich gute Wahl wirkt..\\
In diesem Fall sind die sich daraus ergebenden Vorteile zweierlei. Erstens, ist es dadurch einfacher, nahe an einer Automaten-Spezifikation zu bleiben. Man behällt die Sichtweise auf das System als Zustand und einer Menge an möglichen Aktionen/Zustandsübergängen. Durch die Definition eines Standardverhaltens, nämlich zuerst alle fertigen Aufträge zu Beenden, dann Aufträge zu starten, bis keine Aufträge mehr startbar sind, und dann eine Zeiteinheit vergehen zu lassen, erzielt man ein deterministisches Verhalten (solange die Scheduling Alorithmen deterministisch sind). Dieses reproduzierbare Verhalten ist gegeüber der Prozessorientierten sicht ein Vorteil, da Fehler leichter gefunden und behoben werden können.\\
Zweitens leitet sich aus deterministischem Verhalten ein großer Vorteil für das Property Based Testing zum Erkenntnisgewinn \ref{proptest} ab.\\
Es werden "flaky Tests"(def flakie einfügen) verhindert, also Tests, die mit dem selben Systemzustand bei mehrfacher Ausführung sowohl fehlschlagen als auch gelingen. In diesen Fällen müssten sonst mehrere Tests durchgeführt werden.\\
Was das Shrinking (ref einfügen) angeht, könnte hier ein  Aktivitäts Orientierter Ansatz Sinn ergeben, da immer kleinere Läufe erzeugt werden, sodass sich der Zusatzaufwand des Ereignis Orientierte Paradigmas immer weniger lohnt. Jedoch ist der Overhead so gering, dass er im Vergleich zu den hohen Laufzeiteinbußen in der Ausführung von natürlichen Auftragslisten nicht Ausschlag gebend ist.\\
Darüberhinaus lässt sich durch eine Alternative zum Standardverhaltens leicht untersuchen, wie sich das Verhalten des System ändert, wenn ein Perfektes Betreiben des Klusters nicht möglich ist. Zum Beispiel kann überprüft werden, wie stark sich eine gröbere Zeitauflösung (etwa einmal alle 60 Sekunden) auf das Simulationsergebnis auswirkt. Mehr dazu in \ref{simErrors}.


\subsection{Vergleich von Läufen}
\subsubsection{Geschwindigkeit der Knoten}
Um abschätzen zu können, ob die vorgestellte Simulation erfolgreich reproduziert werden konnte, werden zu erst die Auswertungen von rein sequentiellen Auftragszusammenstellungen verglichen. Obwohl die Parameter zur Erstellung der Aufträge angegeben wurden, sowie die Tatsache, dass alle Knoten mit der selben Geschwindigkeit operieren, fehlt die Angabe über diese Geschwindigkeit. Experimentell lässt sich ermitteln, dass dieser Wert etwa $100$ beträgt.

\subsubsection{LPT,SPT}
\label{spt-lpt-time}
In \cite{Arn99} wird zwischen der Bearbeitungszeit ("processing time") und der Laufzeit ("runtime") eines Auftrags unterschieden. Dabei bezeichnet Laufzeit den "Quotienten aus der Bearbeitungszeit $p_j$ und der kummulativen Geschwindigkeit der zugewiesenen Knoten". Das Ergebnis wird hochgerundet.\\
Bei der Simulation von SPT und LPT Läufen entstand folgendes Problem: Die Ergebnisse von \cite{Arn99} ließen sich korrekt Reproduzieren, solange alle Aufträge sequentiell waren. Sobald auch parallele Aufträge untergemischt wurden, entstanden geringe Abweichungen. (bild einfügen)\\
Es scheint, als würde der SPT Algorithmus in \cite{Arn99} nicht den Auftrag $j$ mit der geringsten Bearbeitungszeit $p_j$ auswählen, sondern denjenigen, der die geringste Zeit in Anspruch nimmt. Zum Beispiel würde $j_1$ mit $p_{j_1} = 10, \pi_{j_1} = 5$ gegenüber $j_2$ mit $p_{j_2} = 4, \pi_{j_1} = 1$ vorgezogen werden, da $j_1$ für $10/5 = 2$  Zeiteinheiten und $j_2$ für $4/1 = 4$ Zeiteinheiten ausgeführt wird.\\
Dies erfordert eine Einfach Apassung bzw. Interpretation des SPT Algorithmus. Es wird nach genannter Bearbeitungszeit geteilt durch den jeweiligen Grad an Parallelität sortiert. Diese Anpassung ergibt allerdings nur Sinn, solange alle Knoten mit der selben Geschwindigkeit arbeiten. Sobald Knoten unterschiedliche Geschwindigkeitseigenschaften besitzen, muss man die Bearbeitungszeit $j_p$ durch die Summe der Geschwindigkeiten der schnellsten freien Knoten teilen, um ein korrektes Ergebnis zu erhalten. Jedoch kann nun nicht bestimmt werden, wie lange ein Auftrag laufen wird, wenn nicht genügend Knoten zur Verfügung stehen. Zwar könnte man ähnlich dem Backfilling-Verfahren für jeden Auftrag vorhersehen, wann genügend Knoten bereitstehen werden, und dann die in der Zukunft freien Knoten zur Bestimmung der Laufzeit nutzen. Dies ist aber eine unverhälltnismäßige Verkomplizierung eines eigentlich eleganten Algorithmus.\\
Dieses Problem wird auch in \cite{Arn99} beschrieben, und es wird ohne nähere Erläuterung angegeben, den ''statischen Bedarf'' der Aufträge zur Selektion zu verwenden. Aus diesen Gründen werden in dieser Arbeit LPT und SPT nach dem Quotienten aus Bearbeitungszeit und Parallelität auswählen. Dies erscheint wie eine sinnvole Interpretation von ''statischem Bedarf''.
\subsubsection{Grafischer Abgleich Abbildugen 1,2 und 3}
Bevor das Simulationsmodell erweitert werden kann, sollte zuerst abgeglichen werden, ob das nachgebaute Modell näherungsweise ähnliche Läufe erzeugt. Dazu werden die in \cite{Arn99} beschriebenen Experimente nachgestellt, und die grafischen Auswertungen nebeneinander gestellt.
Da die Untersuche des Random Schedulingalgorithmus im Verlaufe des Papers aufgegeben wird und mehr Läufe benötigt um stabile Werte zu erzeugen, wird dieser hier nicht betrachet.\\
Abbildungnen eins bis drei lassen sich ohne Probleme, abgesehen von den nicht angegeben Geschwindigkeiten der Knoten, reproduzieren.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_1.png}
\caption{Figure 1, Vergleich von Sequentiellen Aufträgen}
\label{figure1}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_2.png}
	\caption{Figure 2, Vergleich von Sequentiellen Aufträgen, Erstellungszeit wird variiert5}
	\label{figure1}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_3.png}
	\caption{Figure 1, Vergleich von Sequentiellen Aufträgen}
	\label{figure3}
\end{figure}

\FloatBarrier

\subsubsection{Unterschiedlich schnelle Knoten ab Abbildung 4}
Ab dem vierten Experiment offenbart sich folgendes Problem: Die Geschwindigkeiten der Knoten variieren. Sie entsprechen den Messungen von "22 Knoten des lokalen Netzwerks". Jedoch werden diese gemessen Geschwindigkeiten weder in der besprochenen, noch in früheren Veröffentlichungen dargelegt (hier andere paper einfügen). Dies stellt eine sehr hohe Hürde zur Reproduktion dar.\\

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_5}
	\caption{Unterschiedlich schnelle Knoten:\\
	75 Knoten mit Geschwindigkeit 275, 25 Knoten mit Geschwindigkeit 825}
	\label{figure5}
\end{figure}

Um die in \ref{chap:higher-order} vorgeschlagenen Scheduler auch in Eperimentaufbau 4 bis 9 antreten zu lassen, wird eine ungefähre Verteilung der Geschwindikeiten geschätzt.
Hier für wird Abbildung 5 \ref{figure5} herangezogen. In diesem Experiment wird die Bearbeitungsspanne (makespan) angegeben. Dadurch lässt sich eine untere Schranke für die gesamte Rechenleistung des Klusters finden. Wenn durch FirstFit jeder Knoten zu jedem Zeitpunkt ausgelastet wäre, müsste die durchschnittliche Rechenleistung aller Knoten im beschriebenen Versuchsaufbau 384 betragen. Dies wird am Wert für einen Auftragsmix aus 50\% sequentiellen Aufträgen abgelesen. 100 Knoten unbekannter Geschwindigkeit benötigen etwa 3000 Sekunden um 500 Aufträge mit einer durschnittlichen Bearbeitungszeit von 230500 Sekunden fertigzustellen. Um die durch die Algorithmen und die online eingehenden Aufträge entstehenden Effizienzeinbußen auszugleichen, wird die Geschwindigkeit der Knoten zusätzlich um 7,5\% erhöht.
\begin{align*}
\frac{250*(51* 10^3 + 410*10^3)*s}{100x} &= 3000s \\
\frac{250*461}{300} &= x
\end{align*}
Die Frage nach der Art der verwendeten Verteilung lässt sich nicht kären. Deswegen wird hier die einfachste Verteilung gewählt: 2 Klassen von Knoten, mit unbekannten Geschwindgkeiten und Anzahlen. Der zweidimensionale Lösungsraum kann durch eine weitere Annahme auf einen eindimensionalen Raum reduziert: Die Menge der langsamen Knoten soll die selbe Gesamtleistung besitzen wie die Menge der schnellen Knoten.\\
Dies führt zu einer größeren Anzahl langsamer und einer geringeren Anzahl schnellerer Knoten. Diese Konstellationen sind sicherlich interessanter als ihre Umkehrung. Gäbe es viele schnelle und wenige langsame Knoten, wären ähnliche Ergebnisse dadurch zu erreichen, die langsamen Knoten wegzulassen, wodurch sich die Situation nicht viel von den zuvor untersuchten unterscheiden würde.\\
Damit bleibt nur eine einzige Variable, der Anteil der langsamen Knoten im Klaster. Der Wert 0,75 erzeugt visuell vergleichbare Ergebnisse.

\FloatBarrier

\subsubsection{Experimente 4 bis 9}
Ab Experiment 4 kann keine Vergleichbarkeit der Modelle mehr sichergestellt werden. Trozdem werden die Versuche nachgestellt, um die Algorithmen in \ref{chap:higher-order} gegen die fünf bekannten antreten zu lassen. Die Auswertung kann in \ref{weiter} gefunden werden.


\section{Weiterführende Untersuchungen}
\label{weiter}

\begin{figure}	
	\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_4_1}
	\caption{Varieren des Anteils von sequenziellen Aufträgen}
	\label{figure_4_1}
\end{figure}
\begin{figure}	
	\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_4_2}
	\caption{Varieren des Anteils von sequenziellen Aufträgen}
	\label{figure_4_2}
\end{figure}
\begin{figure}	
	\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_6_1}
	\caption{Varieren des Anteils von großen parallelen Aufträgen}
	\label{figure_6_1}
\end{figure}
\begin{figure}	
	\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_6_2}
	\caption{Varieren des Anteils von großen parallelen Aufträgen}
	\label{figure_6_2}
\end{figure}
\begin{figure}	
	\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_7_1}
	\caption{Varieren der Zeitspanne des Anmeldens von Aufträgen}
	\label{figure_7_1}
\end{figure}
\begin{figure}	
	\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_7_2}
	\caption{Varieren der Zeitspanne des Anmeldens von Aufträgen}
	\label{figure_7_2}
\end{figure}
\begin{figure}	
	\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_8_1}
	\caption{Varieren des erlaubten relativen Fehlers in der Angabe der Laufzeit}
	\label{figure_8_1}
\end{figure}
\begin{figure}	
	\includegraphics[width=\textwidth]{/home/hendrik/Programming/Python/BA/Docs/Hendrik/BA_Template/images/Figure_8_2}
	\caption{Varieren des erlaubten relativen Fehlers in der Angabe der Laufzeit}
	\label{figure_8_2}
\end{figure}


\FloatBarrier
\subsection{Backfilling und Fit als Funktionen höherer Ordnung}
\label{chap:higher-order}

Die in Kapitel 2 vorgestellten Funktionen FirstFit und Backfilling sind nur Abwandelungen der FiFo Funktion. Beide wählen den nächsten Auftrag abhängig vom Einreihungszeitpunkt $q_j$, jedoch unter Einschränkungen. "FiFo-Fit" wählt den ersten unter den startbaren Aufträgen aus, "FiFo-BackFilling" wählt den ersten Auftrag, oder einen, der den Bearbeitungsbeginn des ersten Auftrags nicht verzögert. "-Fit" und "-Backfilling" fügen also einen relevanten Kontext hinzu. Es liegt nahe, diese Erweiterungen als Funktionen höherer Ordnung zu betrachten. Ihre Domäne besteht aus einer Scheduling Funktion, den wartenden Aufträgen und einen Kontext - Anzahl an verfügbaren Knoten oder die Abschlusszeitpunkte und die Parallelität der laufenden Aufträge -, und bilden diese partiell auf einen Auftrag ab. 
Da FirstFit-Backfilling von [PaperZitat] als Sieger auserkohren wurde, LPT und GPT aber nicht im Backfilling Kontext untersucht wurden, wird im folgenden untersucht, ob LPT oder GPT mit -FirstFit oder -Backfilling ähnlich gute Ergebnisse erzielen können.\\
In \ref{proptest} wird eine Abwandlung des Backfillings vorgestellt, das optimistische Backfilling. Dieses wird hier auch mit untersucht, allerdings nur im FiFo Kontext.

\paragraph{SPT}
In Abbildung \ref{figure_4_1} wird die durschnittliche Zeit eines Auftrags im System gemessen. Dabei schneidet SPT-Fit besser ab als alle anderen Algorithmen. Den zweiten Platz belegt FiFo-Fit. Für die durschnittliche Systemzeit ist es wichtig, so schnell wie möglich Aufträge fertig zu stellen. Deshalb eigenen sich ''-Fit'' Funktionen, besonders in Kombination mit SPT. Erstaunlicherweise ist ''-Fit'' jedoch im Bezug zur Systemzeit nicht vollständig monoton. So gilt für die Messungen zwar SPT $<$ LPT $<$ FiFo, jedoch Fit$($FiFo$)$ $<$ Fit$($LPT$)$.\\
In Abbildung \ref{figure_7_2} erzielt SPT-Fit die selben Werte wie FiFo-Fit und LPT-Fit. In diesem Experiment wird die Bearbeitungsspanne in Abhängigkeit von einer relativen Fehlerrate in der Angabe von Bearbeitungszeiten ermittelt.\\


\paragraph{LPT}
In Abbildung \ref{figure_6_1} gibt es eine Klasse an Algorithmen, die gleich gute Ergebnisse erzielen. Darunter FiFo-Fit, SPT-Fit, LPT-Fit, LPT-Backfill und FiFo-Optimistic-Backfill. In diesem Experiment wird die Bearbeitungsspanne in Abhängigkeit des anteils von großen parallelen Aufträgen gemessen. LPT-Fit ist hier deshalb eine gute Wahl, da LPT im Vergleich zu SPT und FiFo für eine geringere Spanne sorgt (vgl. auch \ref{figure3}). ''-Fit'' und ''-Backfill'' sorgt dafür, dass lange Phasen zum Anfang des Laufs, in denen einige Knoten nicht genutzt werden, während lange Aufträge abgearbeitet werden, gefüllt werden können.\\
Diese Beobachtung wiederholt sich in Abbildung \ref{figure_7_1}. Auch hier wird die Bearbeitungsspanne als Maß angelegt.\\

\paragraph{Optimistisches Backfilling}
Dieser Algorithmus wird in \ref{optfill} vorgestellt.\\
Beim Optimistischen Backfilling stellt sich vorallem die Frage, wie groß der Unterschied zum ''normalen'' Backfilling ist, und ob es ein Unterschied zu unseren Gunsten ist. Hierfür wurde nur FiFo in beiden Kontexten herangezogen.\\
In \ref{figure_4_1} bietet das Optimistische Verfahren einen leichten, jedoch erkennbaren Vorteil Hier wird die durchschnittliche Systemzeit von Aufträgen in Abhängigkeit des Anteils sequentieller Aufträge ermittelt. Es erscheint nachvollziebar, dass, vorausgesetzt es gibt etwa gleich viele sequezielle als auch parallele Aufträge, eine Leistungssteigerung durch das Optimistische Verfahren erzielt werden kann. Da dieses weniger restriktiv ist, welche Aufträge gestartet werden dürfen, um Lücken zu füllen.\\
So auch in \ref{figure_7_1}. Hier wird die gesamte Bearbeitungszeit in Abhängigkeit eines erlaubten Fehlers untersucht. Die selbe Argumentation erscheint plausibel. Wenn die Bearbeitungszeit eines Auftrags nicht korrekt angegben wurde, werden weniger Aufträge gestartet um Lücken zu füllen. Das Optimistische Backfilling darf trotzdem Aufträge die wenige Knoten benötigen starten, und nutzt so die Freiräume besser aus.\\
In allen anderen Versuchen ist kein nennenswerter Unterschied feststellbar. Es erscheit, als wären die in \ref{optfill} genannten Konstellationen selten genug, als dass sie die nicht genutzten Chancen des ''normalen'' Backfilling überwiegen. Zumindest in den hier untersuchten Experimenten lässt sich die optimistische Variante als Ersatz empfehlen. In den meisten Fällen ist kein deutlicher Unterschied erkennbar, aber wenn, dann immer zu unseren Gunsten. Die Auswirkung auf LPT und SPT könnten in Zukunft untersucht werden. 

\FloatBarrier

\subsection{Property Based Testing zum Erkenntnisgewinn}
\label{proptest}
\paragraph{Testen statt Simullieren}
Während das Simullieren des Rechenclusters gute Auskunft darüber gibt, wie sich das System bei einer langen Reihe an Aufträgen verhällt, kristalliesiert sich dabei lediglich das durchschnittliche Verhalten heraus. Obwohl diese Analyse sinnvoll ist, um verschiedene Scheduling Funktionen asymptotisch mit einander zu vergleichen, wird durch das Mitteln von hunderten von Läufen nicht ersichtlich, in welchen Fällen eine normalerweise unterlegene Scheduling Funktion einer Anderen überlegen ist.\\
Um eine Intuition für die Unterschiede zwischen Scheduling Funktionen entwickeln zu können ist es hilfreicher, die durch kleinstmögliche Listen an Aufgaben erzeugten Läufe zu vergleichen. Um diese minimalen Beispiele zu generieren, kann ein Property Based Testing Framework verwendet werden. Ein guter Einstiegspunkt in dieses Thema ist [initial Paper 1999]. Hier nur eine kleine Zusammenfassung der für uns wichtigen Aspekte. \\
Ein Test besteht aus einer zu testenden Funktion, eine Eigenschaft, die die Ausgabe der Funktion haben soll, und einem Generator, der Eingaben produziert. Sobald eine Eingabe gefunden wird, deren Ausgabe die geforderte Eigenschaft nicht erfüllt, wird die Eingabe automatisch geschrumptf. Dies führt zu einem leichter interpretierbaren Beispiel, da der ausgegebene Fall "kein Rauschen" enthällt.
Eine Zahl wird geschrumpt, in dem sie verringer wird, ein Tupel von Zahlen, in dem eines der Elemente geschrumpft wird, und eine Liste, in dem Elemnter der Liste weggelassen oder geschrumpft werden.\\
Um ein minimales Beispiel zu finden, in dem Scheduling Funktion $S_1$ Ziel Funktion $T$ besser minimiert als Scheduler $S_2$, können wir die Eigenschaft $T(S_1(x)) >= T (S_2(1))$, mit einem Auftragsgenerator$X$: $(Anzahl Konten,[(\pi_j <= Anzahl Knoten,p_j,q_j)])$ überprüfen.
Ein vom Testframework gefundenes minimales Gegenbeispiel, zeigt uns einen Speziellen Fall, in dem $S_1$ ein besseres Ergebnis erzielt als $S_2$
Für die meinsten Scheduling Funktionen ist das minimal Beispiel eines mit 3 Aufträgen, und einer kleinen Anzahl an Knoten [genaues nochmal testen].

Hier nun einige Beispiele:\\

\paragraph{Optimistisches Backfilling}
\label{optfill}
Das vorgestellte ''-Backfilling'' Verfahren wirkt auf den ersten Blick zurückhaltend. Das angegebene Ziel, die zum ''-Fit'' verglichene Wartezeit gering zuhalten, wird erfüllt, in dem große Aufträge nicht benachteiligt werden. Kleine werden nur vorgezogen, wenn sich dadurch die Wartezeit des besten, aber nicht startbaren Kandidaten, nicht verzögert. Dies wird erreicht, in dem ein Auftrag $P'$ nur starten darf, wenn er abgeschlossen wird, bevor der beste Kandidat $P$ startet.\\
Warum aber darf ein Auftrag $P'$, der wenige Knoten benötigt, nicht starten, vorausgesetzt, er nimmt nur so wenige Knoten in Anspruch, dass $P$ wie geplant starten kann ($P$ benötigt noch $n$ Knoten zum starten, sobald er starten wird, sind aber $n+k, k>0$ Knoten frei, und $\pi_{P'} <= k$).\\
Es ist intuitiv, des es einen Haken gibt. Ohne in Scheduling Theorie geübt zu sein, ist es aber nicht einfach, aus dem Stand ein Beispiel zu konstruieren, in dem sich das optimistische -Backfilling negativ auf die Wartezeit auswirkt. Allerdings kann dank Property Based Testing ohne viel Mühe eines in wenigen Sekunden generiert werden.

\begin{figure}
\centering
\begin{verbatim}
Is fifo\_optimistic allways better than fifo\_backfill by maximumLateness?
No! counterexample:
queueintT, processingT, realProcessingT, degreeOfParallelism
id: 0, qT: 1, pT: 3, rPT: 3, doP: 1
id: 1, qT: 0, pT: 3, rPT: 3, doP: 2
id: 2, qT: 0, pT: 1, rPT: 1, doP: 2
id: 3, qT: 0, pT: 1, rPT: 1, doP: 3

maximumLateness of fifo\_optimistic: 4
[0]:112|-3
[1]:112|-3
[2]:-00|03

maximumLateness of fifo\_backfill: 3
[0]:112|3--|-
[1]:11-|300|0
[2]:--2|3--|-
\end{verbatim}
\caption{Vergleich vom Normalem und Optimistischem Backfilling}
\label{onlateness}
\end{figure}

\FloatBarrier

Hier ist der Optimistische Algorithmus zu vorfreudig. Auftrag 0 wird gestartet, obwohl er (als einziger) nicht seit beginn angemeldet ist. Dadurch wird Auftrag 3 nach hinten geschoben, und die maximale Wartezeit für 3 steigt. Trozdem is die Auslastung des optimistischen Algorithmus deutlich besser. Der findet man auch einen Fall, in dem sowohl die maximale Verspätung, als auch die gesamt Bearbeitungszeit schlechter sind?\\


\begin{figure}
\centering
\begin{verbatim}
Is fifo_optimistic allways better than <function System.fifo_backfill at 0x7f42574fd7b8> by <function maximumLateness at 0x7f42408ff400> ?
No! counterexample:

queueintT, processingT, realProcessingT, degreeOfParallelism
id: 0, qT: 1, pT: 1, rPT: 1, doP: 4
id: 1, qT: 1, pT: 3, rPT: 3, doP: 1
id: 2, qT: 1, pT: 3, rPT: 3, doP: 1
id: 3, qT: 0, pT: 4, rPT: 4, doP: 3
id: 4, qT: 0, pT: 1, rPT: 1, doP: 2

maximumLateness offifo_optimistic: 4
makespan of fifo_optimistic: 8
[0]:334|-0-|--
[1]:334|-0-|--
[2]:33-|-02|22
[3]:-11|10-|--

maximumLateness offifo_backfill: 3
makespan of fifo_backfill: 7
[0]:334|0--|-
[1]:33-|011|1
[2]:33-|022|2
[3]:--4|0--|-
\end{verbatim}
\caption{Vergleich vom Normalem und Optimistischem Backfilling in Spanne und Verspätung}
\label{onlatenessmakespan}
\end{figure}

Natürlich. Auch hier kann ein durch Property Based Testing ein Fall konstruiert werden. Allerdings besteht hier das kleinste gefundene Beispiel bereits aus 5 Aufträgen. Es ist also zu erwarten, dass solche Konstellationen selten genug sind. Eine genauere Untersuchung dazu im Abschnitt \ref{chap:higher-order}
Dauert ca ne Minute auf crappy laptop\\
\FloatBarrier

Wie in der Grafik \ref{onlateness} nach einem Schritt erkennbar, benachteiligt ein solcher optimistisch gestarteter langer, kleiner Auftrag nicht den besten Kandidaten (Auftrag 4), allerdings den zweitbesten Kandidaten (Auftrag 0). Ob dies ein seltener Fall ist, oder ob sich Optimismus im Mittel auszahlt, kann wiederum durch eine Statistische Asuwertung untersucht werden.\\

Außerdem ist es interessant zu beobachten, welche Informationen implizit aus diesem Gegenbeispiel gezogegen werden können. Zum Beispiel, dass im ersten Lauf \ref{onlateness} nur ein einziger online Auftrag (d.h. nicht schon zum Zeitpunkt 0 bekannt) nötig ist, aber um das optimistische Verfahren in beiden Metriken zu schlagen, werden 3 online Aufträge sowie ein zusätzlicher Auftrag benötigt. Es fällt auch auf, dass in diesem Beispiel FiFo-Backfill nie vom Backfilling Gebrauch macht. Es verhällt sich immer wie das blanke FiFo Verfahren. Warum also nicht einen Auftragsmix finden, in dem das normale Backfilling sowohl FiFo als auch das optimistische Backfilling trumpf, und zwar sowohl was die Bearbeitungsspanne als auch die längste Wartezeit angeht? Auch hier soll der Leser aufgefordert sein, selber nach einem Beispiel zu suchen.\\

\begin{figure}
	\centering
	\begin{verbatim}
id: 0, qT: 0, pT: 1, rPT: 1, doP: 2
id: 1, qT: 0, pT: 1, rPT: 1, doP: 3
id: 2, qT: 0, pT: 2, rPT: 2, doP: 1
id: 3, qT: 0, pT: 1, rPT: 1, doP: 3
id: 4, qT: 0, pT: 1, rPT: 1, doP: 2

maximumLateness of fifo_optimistic : 3
makespan of fifo_optimistic : 4
[0]:013|4
[1]:013|4
[2]:223|-
[3]:-1-|-

maximumLateness of fifo_backfill : 2
makespan of fifo_backfill : 3
[0]:013
[1]:013
[2]:413
[3]:422

maximumLateness of fifo : 3
makespan of fifo : 4
[0]:013|4
[1]:013|4
[2]:-13|-
[3]:-22|-
\end{verbatim}
\caption{Backfill besser als Optimistic und Fifo, bez. Bearbeitunsspanne und längster Wartezeit}
\label{everything}
\end{figure}

Des weiteren fällt auf, dass das Shrinking Verfahren herausgefunden hat, dass zur Berechnung der Laufzeiten von parallelen Aufträgen hochgerundet wird (s. \ref{spt-lpt-time}). Da dies der Lesbarkeit der Tests zuwider läuft, kann die Bearbeitungszeit jedes Auftrags einfach auf das nächste Vielfache der Parallelität gesetzt werden.
Zusätlich wird oft die Gleichstände-auflösenden Ids zurückgegriffen. Es kann explizit gefordert werden, dass alle Einreihungszeitpunkte der Aufträge verschieden sind. Allerdings verlangsamt dies die Suche nach einem Gegenbeispiel, und das kleinste gefundene Gegenbeispiel hat eine Bearbeitungsspanne von 13 Zeiteinheiten oder mehr.\\
\FloatBarrier

\subsection{Simulation von Fehlerhaftem Verhalten}
\label{simErrors}

\paragraph{Lorem}
ipsum